---
permalink: /
title: "â›±"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm an AI researcher currently working in Professor [Tan Nguyen](https://tanmnguyen89.github.io/)'s (National University Singapore) mathematical AI lab, as part of FPT AI Center's research residency. 

# Research Overview

I work on the statistical foundations underpinning frontier mechanisms, such as self-attention and mixture-of-experts, and leverage statistical frameworks and methods to interpret and improve these models.

Recently, I've applied this approach to improve the representation capacity and robustness of the self-attention mechanism, where I used a nonparametric kernel regression framework to propose using hyper-ellipsoidal attention regions [(NeurIPS, 2024)](https://arxiv.org/pdf/2406.13770). I have also examined mixture-of-experts (MoE) Transformers through the lens of feature-weighted clustering optimization to propose a novel router that improves expert specialization, and promotes faster convergence and better robustness [(ICLR, 2025)](https://openreview.net/pdf?id=Pu3c0209cx). Going further back, I've also worked on [an attention-based framework](https://arxiv.org/abs/2411.14765) for fair representation learning that goes beyond binary categorizations of fairness.

Going forward, I'm hoping to expand on my research experience in a few different ways, and these are a few questions I'm thinking about:

- How do behaviors in LLMs, in particular reasoning and memorization, emerge from architectural choices?
- Are there ways to leverage statistical regularities in the data or model internals to design better mechanisms, in particular attention and MoE routing?
- How do these statistical frameworks underpinning core mechanisms change when moving into multimodal settings, and what opportunities are there to use them for better multimodal models?
- What are the statistical properties in pre and post-training that account for misalignment in LLMs?
- How do harmful concepts and undesirable behavior (such as dishonesty and power-seeking) show up in model internals?
- How can we leverage existing capabilities in LLMs to self-improve and self-align?

Prior to working with Professor Nguyen, I studied statistics at Columbia, where I was a funded research scholar working with Professor [Talia Gillis](https://www.law.columbia.edu/faculty/talia-gillis) on fair machine learning, and economics at Cambridge University and the London School of Economics.


<!-- I study the **statistical foundations of frontier models**, in particular Transformers, where I aim to leverage theoretical frameworks to understand, interpret, and improve these models.  -->

<!-- My research perspective is that frontier models and mechanisms may change, but the underlying mathematical principles that enable learning are (more) constant, and so through understanding the foundations are we well positioned to make consistent and lasting improvements to deep learning, no matter how the field continues to evolve.  -->

<!-- Recently, I've applied this approach to improve the representation capacity and robustness of the self-attention mechanism, where I used a nonparametric kernel regression framework to propose using hyper-ellipsoidal attention regions [(NeurIPS, 2024)](https://arxiv.org/pdf/2406.13770). I have also examined mixture-of-experts (MoE) Transformers through the lens of feature-weighted clustering optimization to propose a novel router that improves expert specialization, and promotes faster convergence and better robustness [(ICLR, 2025)](https://openreview.net/pdf?id=Pu3c0209cx). Going further back, I've also worked on [an attention-based framework](https://arxiv.org/abs/2411.14765) for fair representation learning that goes beyond binary categorizations of fairness. -->

<!-- For future directions, I'm hoping to build on my research experience towards looking at statistically grounded approaches to **finetune** and **merge** LLMs, as well as how to how to **align** and **mechanistically interpret** them.  -->


<!-- For future directions, I'm hoping to expand my research experience, that has predominantly centered on model-driven pretraining approaches, towards also including in-context learning, finetuning, and model merging. I also have a growing interest in alignment and mechanistic interpretability. Regardless of the direction, however, I hope to conduct research that leverages rigorous, statistical frameworks. -->


# Publications

[Tight Clusters Make Specialized Experts](https://openreview.net/pdf?id=Pu3c0209cx) \
**Stefan Nielsen**, Rachel Teo, Laziz Abdullaev, Tan Minh Nguyen \
*International Conference on Learning Representations (ICLR), 2025*

[Elliptical Attention](https://arxiv.org/pdf/2406.13770) \
**Stefan Nielsen**, Laziz Abdullaev, Rachel Teo, Tan Minh Nguyen \
*Advances in Neural Information Processing Systems (NeurIPS), 2024*

[An Attention-based Framework for Fair Contrastive Learning](https://arxiv.org/abs/2411.14765) \
**Stefan Nielsen**, Tan Minh Nguyen \
*preprint, 2023*

# Service
- **Reviewer**, ICLR Main Conference, 2025
- **Reviewer**, ICLR Workshop on Modularity for Collaborative, Decentralized, and Continual Deep Learning, 2025


# Other
ðŸ““ I'm glad to have recieved the Huayu Enrichment Scholarship, which is a fully funded scholarship with stipend to study mandarin chinese at National Taiwan University for three months.

ðŸŒ± I'm also proud to have been a contributing artist and producer of [Well Settled](https://www.britishcouncil.vn/cac-chuong-trinh/uk-vietnam-season-2023/projects/shared-heritage-vn/well-settled-activating-viet), a project funded by Hackney Arts Council to produce artworks exploring Vietnamese identity in transnational contexts as part of the larger activation of the An Viá»‡t Archives.